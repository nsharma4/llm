{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from langchain.docstore.document import Document\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Authenticate with the Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the \"All The News\" dataset from Kaggle\n",
    "api.dataset_download_files('asad1m9a9h6mood/news-articles', path='data/', unzip=True)\n",
    "\n",
    "# Load and process the dataset\n",
    "file_path = 'data/Articles.csv'  # Adjust the path based on dataset name\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert documents into the required format for Chroma\n",
    "document_list = [\n",
    "    Document(\n",
    "        page_content=row['Article'],\n",
    "        metadata={\n",
    "            'date': row['Date'],\n",
    "            'heading': row['Heading'],\n",
    "            'news_type': row['NewsType']\n",
    "        }\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# TODO Truncate document list because it takes too long to process\n",
    "if len(document_list) > 1000:\n",
    "    document_list = document_list[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed documents using SentenceTransformer\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Create a vector store using Chroma\n",
    "vector_store = Chroma.from_documents(document_list, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a basic query 2.1\n",
    "query = \"Tell me about the latest news in the world and include the date\"\n",
    "\n",
    "# Perform semantic search and retrieve relevant documents\n",
    "retrieved_docs = vector_store.similarity_search(query, k=5)\n",
    "\n",
    "# Combine retrieved documents into a context string\n",
    "context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Initialize the LLM (using GPT-2 as an example)\n",
    "llm = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate the final response\n",
    "response = llm(context, max_new_tokens=100, do_sample=True, top_k=50)[0]['generated_text']\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of 2.2\n",
    "# Function to expand the query using WordNet\n",
    "\n",
    "def expand_query_with_synonyms(query):\n",
    "    synonyms = set()\n",
    "    for word in query.split():\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "    expanded_query = query + \" \" + \" \".join(synonyms)\n",
    "    return expanded_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the initial query\n",
    "expanded_query = expand_query_with_synonyms(\"Tell me about the latest news in AI\")\n",
    "\n",
    "# Perform semantic search and retrieve relevant documents\n",
    "retrieved_docs = vector_store.similarity_search(expanded_query, k=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rank documents by relevance\n",
    "def rank_documents_by_relevance(docs, query_embedding):\n",
    "    ranked_docs = sorted(\n",
    "        docs, key=lambda doc: vector_store.similarity(query_embedding, doc.page_content), reverse=True\n",
    "    )\n",
    "    return ranked_docs\n",
    "\n",
    "# Obtain the query embedding and rank the retrieved documents\n",
    "query_embedding = embedder.encode(expanded_query)\n",
    "ranked_docs = rank_documents_by_relevance(retrieved_docs, query_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine top-ranked documents into a context string\n",
    "context = \" \".join([doc.page_content for doc in ranked_docs[:2]])  # Limiting to top 2 documents\n",
    "\n",
    "# Initialize the LLM (using Mistral or GPT-2 as an example)\n",
    "llm = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-v0.1\", token=\"your_huggingface_token\")\n",
    "\n",
    "# Generate the final response\n",
    "response = llm(context, max_new_tokens=100, do_sample=True, top_k=50)[0]['generated_text']\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
