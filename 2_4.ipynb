{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"9oUAKSu5updk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Read the token from the file\n","token_path = '/content/drive/MyDrive/github_token.txt'\n","with open(token_path, 'r') as token_file:\n","    github_token = token_file.read().strip()\n","\n","%cd /content/drive/MyDrive/llm//\n","\n","# Define the GitHub repository URL with the token for authentication\n","repo_url = f\"https://{github_token}@github.com/nsharma4/llm.git\"\n","\n","\n","# Update the remote URL to include the token\n","!git -C /content/drive/MyDrive/llm/ remote set-url origin {repo_url}\n","\n","!git config --global user.email \"nealsharma99@gmail.com\"\n","!git config --global user.name \"nsharma4\"\n","\n","\n"],"metadata":{"id":"E4QHb_c0gsIe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","!pip install -r requirements.txt\n"],"metadata":{"id":"chU_R2WZ5JPS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GiZHjLBIYv2L"},"outputs":[],"source":["import os\n","import pandas as pd\n","import langchain\n","import nltk\n","from kaggle.api.kaggle_api_extended import KaggleApi\n","from langchain.vectorstores import Chroma\n","from langchain.embeddings import SentenceTransformerEmbeddings\n","from sentence_transformers import SentenceTransformer\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from langchain.docstore.document import Document\n","from nltk.corpus import wordnet\n","import torch\n","\n","# Ensure NLTK resources are downloaded\n","nltk.download('wordnet')\n","\n","# Authenticate with the Kaggle API\n","api = KaggleApi()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJ04RrREYv2O"},"outputs":[],"source":["# Download the \"All The News\" dataset from Kaggle\n","api.dataset_download_files('asad1m9a9h6mood/news-articles', path='data/', unzip=True)\n","\n","# Load and process the dataset\n","file_path = 'data/Articles.csv'  # Adjust the path based on dataset name\n","df = pd.read_csv(file_path, encoding='ISO-8859-1')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMfcPtWPYv2P"},"outputs":[],"source":["# Convert documents into the required format for Chroma\n","document_list = [\n","    Document(\n","        page_content=row['Article'],\n","        metadata={\n","            'date': row['Date'],\n","            'heading': row['Heading'],\n","            'news_type': row['NewsType']\n","        }\n","    )\n","    for _, row in df.iterrows()\n","]\n","\n","# NOTE Truncate document list because it takes too long to process\n","max_length = 1000\n","if len(document_list) > max_length:\n","    document_list = document_list[:max_length]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmublekcYv2P"},"outputs":[],"source":["# Initialize the SentenceTransformer and Chroma vectorstore\n","# Embed documents using SentenceTransformer\n","\n","embedding_function = SentenceTransformerEmbeddings(model_name = 'all-MiniLM-L6-v2')\n","\n","# Create a vector store using Chroma\n","vector_store = Chroma.from_documents(document_list, embedding_function)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtd1u8tgYv2P"},"outputs":[],"source":["def expand_query(query):\n","    expanded_terms = []\n","    for word in query.split():\n","        synsets = wordnet.synsets(word)\n","        if synsets:\n","            expanded_terms.extend([lemma.name() for lemma in synsets[0].lemmas()])\n","    return \" \".join(set(query.split() + expanded_terms))\n","\n","def dynamic_retrieval(query, docs, vector_store, k=2):\n","    expanded_query = expand_query(query)\n","    new_docs = vector_store.similarity_search(expanded_query, k=k)\n","    retrieved_docs = {doc.metadata['heading'] for doc in docs}\n","    unique_new_docs = [doc for doc in new_docs if doc.metadata['heading'] not in retrieved_docs]\n","    return unique_new_docs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_yfwNJPmYv2P"},"outputs":[],"source":["# Set the correct model path\n","model_path = \"/content/drive/MyDrive/llm/Mistral-7B-v0.1\"\n","\n","# Load the model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n","tokenizer = AutoTokenizer.from_pretrained(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZM8V9DjYv2Q"},"outputs":[],"source":["# Expand the initial query\n","query = \"Tell me about what the news in Karachi is based on what you know\"\n","\n","# Initial document retrieval using the query\n","current_docs = vector_store.similarity_search(query, k=2)\n","context = \" \".join([doc.page_content for doc in current_docs])\n","\n","# Caching tokenized context to avoid re-tokenization\n","cached_inputs = tokenizer(context, return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQaxJz9hYv2Q","collapsed":true},"outputs":[],"source":["max_iterations = 3\n","for i in range(max_iterations):\n","    # Generate text\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            cached_inputs['input_ids'],\n","            max_new_tokens=40,\n","            do_sample=True,\n","            top_k=50,\n","            top_p=0.95,\n","            temperature=0.7,\n","            length_penalty=1.5\n","        )\n","\n","    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    print(f\"Iteration {i+1} generated text:\\n{generated_text}\\n\")\n","\n","    # Check if more information is needed\n","    if len(generated_text.split()) < 20 or \"need more information\" in generated_text.lower():\n","        new_docs = dynamic_retrieval(generated_text, current_docs, vector_store, k=2)\n","        if new_docs:\n","            current_docs += new_docs\n","            new_context = \" \".join([doc.page_content for doc in new_docs])\n","            context += \" \" + new_context\n","            cached_inputs = tokenizer(context, return_tensors=\"pt\")\n","        else:\n","            print(\"No new relevant information found. Ending generation.\")\n","            break\n","    else:\n","        print(\"Sufficient information generated. Ending generation.\")\n","        break\n","\n","# # Cell 10: Clean up (optional)\n","# del model\n","# torch.cuda.empty_cache()"]},{"cell_type":"code","source":["!git -C /content/drive/MyDrive/llm/ rm -r --cached .\n","!git -C /content/drive/MyDrive/llm/ add .\n","\n","!git -C /content/drive/MyDrive/llm/ commit -m \"clea\"\n","!git push\n","\n"],"metadata":{"id":"dWvbz80Pn6Eo"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}